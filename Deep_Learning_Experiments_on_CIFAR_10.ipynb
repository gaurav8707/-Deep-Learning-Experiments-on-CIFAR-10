{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning Experiments on CIFAR-10\n",
        "\n",
        "\n",
        "*   Name - Gaurav Sharan\n",
        "*   Registration Number - 20MIA1081\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FDy-Sb7Aq0HZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRH64OGWE_rY",
        "outputId": "70036e28-5dd1-470f-9a9b-5c01fbb67a97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.25.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.20.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F  # For activation functions\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import wandb\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Log in to Weights & Biases\n",
        "wandb.login()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "M9qwx-kTFZ_3",
        "outputId": "401f663f-5e6f-41bc-fb31-fbb130edc855"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mg22168559\u001b[0m (\u001b[33mg22168559-vellore-institute-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1  # For basic block the expansion factor is 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        \"\"\"\n",
        "        A basic residual block for ResNet‑32.\n",
        "        Args:\n",
        "          in_planes: Number of input channels.\n",
        "          planes: Number of output channels.\n",
        "          stride: Stride for the first convolution.\n",
        "        \"\"\"\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        # Define the shortcut connection\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet32(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        \"\"\"\n",
        "        Constructs a ResNet‑32 model for CIFAR‑10.\n",
        "        Args:\n",
        "          block: Block type (BasicBlock).\n",
        "          num_blocks: A list specifying how many blocks each layer should have (for ResNet-32, this would be [5, 5, 5]).\n",
        "          num_classes: Number of output classes (10 for CIFAR‑10).\n",
        "        \"\"\"\n",
        "        super(ResNet32, self).__init__()\n",
        "        self.in_planes = 16\n",
        "\n",
        "        # Initial convolution layer (3×3 conv for CIFAR‑10)\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "\n",
        "        # Creating three layers (groups) of residual blocks\n",
        "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
        "\n",
        "        # Global average pooling and final fully connected layer\n",
        "        self.linear = nn.Linear(64, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        \"\"\"\n",
        "        Creates one group (layer) of residual blocks.\n",
        "        \"\"\"\n",
        "        strides = [stride] + [1]*(num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        # Global average pooling over spatial dimensions\n",
        "        out = F.avg_pool2d(out, out.size(3))\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def get_resnet32_for_cifar10():\n",
        "    \"\"\"\n",
        "    Instantiates a ResNet‑32 model for CIFAR‑10.\n",
        "    For ResNet‑32, we use 5 residual blocks per layer: [5, 5, 5].\n",
        "    Returns:\n",
        "      model (nn.Module): A ResNet‑32 model.\n",
        "    \"\"\"\n",
        "    return ResNet32(BasicBlock, [5, 5, 5], num_classes=10)\n"
      ],
      "metadata": {
        "id": "TOzS8HK4Fzdn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, optimizer, criterion, train_loader, device):\n",
        "    \"\"\"\n",
        "    Trains the model for one epoch.\n",
        "\n",
        "    Args:\n",
        "      model (nn.Module): Model to train.\n",
        "      optimizer: Optimizer.\n",
        "      criterion: Loss function.\n",
        "      train_loader (DataLoader): Training data loader.\n",
        "      device: CPU or GPU.\n",
        "\n",
        "    Returns:\n",
        "      epoch_loss (float): Average training loss.\n",
        "      epoch_acc (float): Training accuracy.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def validate(model, criterion, loader, device):\n",
        "    \"\"\"\n",
        "    Evaluates the model on a validation/test set.\n",
        "\n",
        "    Args:\n",
        "      model (nn.Module): Model to evaluate.\n",
        "      criterion: Loss function.\n",
        "      loader (DataLoader): Validation/test data loader.\n",
        "      device: CPU or GPU.\n",
        "\n",
        "    Returns:\n",
        "      epoch_loss (float): Average loss.\n",
        "      epoch_acc (float): Accuracy.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = correct / total\n",
        "    return epoch_loss, epoch_acc\n"
      ],
      "metadata": {
        "id": "ij6DUPIMF8HX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(config=None):\n",
        "    \"\"\"\n",
        "    Runs a single experiment with the given hyperparameters.\n",
        "    - Each experiment gets logged as a separate run on W&B.\n",
        "    - Checkpoints are saved at the end of each epoch with unique filenames and uploaded.\n",
        "    - Optionally, a MultiStepLR scheduler can be used if specified.\n",
        "\n",
        "    Args:\n",
        "      config (dict): A dictionary containing the hyperparameters for the experiment.\n",
        "\n",
        "    Returns:\n",
        "      model (nn.Module): The trained model after the experiment.\n",
        "      test_loader (DataLoader): The DataLoader for the test dataset, ready for evaluation.\n",
        "    \"\"\"\n",
        "    with wandb.init(project=\"cifar10_classification_Data_science_assignment_Tata_communication\", config=config) as run:\n",
        "        config = wandb.config\n",
        "\n",
        "        # Data transformations for training and testing\n",
        "        transform_train = transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "        ])\n",
        "        transform_test = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "        ])\n",
        "\n",
        "        # Download CIFAR-10 dataset\n",
        "        train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "        test_dataset  = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "        # Create a train/validation split (80% train, 20% validation)\n",
        "        num_train = int(0.8 * len(train_dataset))\n",
        "        num_val = len(train_dataset) - num_train\n",
        "        train_subset, val_subset = torch.utils.data.random_split(train_dataset, [num_train, num_val])\n",
        "\n",
        "        train_loader = torch.utils.data.DataLoader(train_subset, batch_size=config.batch_size, shuffle=True, num_workers=2)\n",
        "        val_loader   = torch.utils.data.DataLoader(val_subset, batch_size=config.batch_size, shuffle=False, num_workers=2)\n",
        "        test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "        # Initialize the ResNet-32 model for CIFAR-10\n",
        "        model = get_resnet32_for_cifar10().to(device)\n",
        "\n",
        "        # Define the loss function\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Select the optimizer based on configuration\n",
        "        if config.optimizer.lower() == \"sgd\":\n",
        "            optimizer = optim.SGD(model.parameters(), lr=config.learning_rate, momentum=0.9, weight_decay=5e-4)\n",
        "        elif config.optimizer.lower() == \"adam\":\n",
        "            optimizer = optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=5e-4)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported optimizer type. Please choose 'sgd' or 'adam'.\")\n",
        "\n",
        "        # Optionally, set up a learning rate scheduler\n",
        "        scheduler = None\n",
        "        if hasattr(config, \"use_scheduler\") and config.use_scheduler:\n",
        "            # Decay learning rate at 50% and 75% of total epochs\n",
        "            milestones = [int(config.epochs * 0.5), int(config.epochs * 0.75)]\n",
        "            scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.1)\n",
        "\n",
        "        best_val_acc = 0.0  # To track the best validation accuracy\n",
        "\n",
        "        # Training loop for the specified number of epochs\n",
        "        for epoch in range(config.epochs):\n",
        "            train_loss, train_acc = train_one_epoch(model, optimizer, criterion, train_loader, device)\n",
        "            val_loss, val_acc = validate(model, criterion, val_loader, device)\n",
        "\n",
        "            # Log metrics to W&B for the current epoch\n",
        "            wandb.log({\n",
        "                \"epoch\": epoch + 1,\n",
        "                \"train_loss\": train_loss,\n",
        "                \"train_acc\": train_acc,\n",
        "                \"val_loss\": val_loss,\n",
        "                \"val_acc\": val_acc,\n",
        "                \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
        "                \"optimizer\": config.optimizer\n",
        "            })\n",
        "\n",
        "            print(f\"Epoch [{epoch+1}/{config.epochs}] | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "            # Save every epoch's checkpoint with a unique filename\n",
        "            checkpoint_filename = f\"{wandb.run.id}_epoch_{epoch+1}_{int(time.time())}.pth\"\n",
        "            torch.save(model.state_dict(), checkpoint_filename)\n",
        "            wandb.save(checkpoint_filename)\n",
        "\n",
        "            # Optionally, save the best model checkpoint separately if validation accuracy improves\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                best_checkpoint = f\"{wandb.run.id}_best_epoch_{epoch+1}_{int(time.time())}.pth\"\n",
        "                torch.save(model.state_dict(), best_checkpoint)\n",
        "                wandb.save(best_checkpoint)\n",
        "\n",
        "            # Step the learning rate scheduler, if used\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "\n",
        "        # After training, evaluate the model on the test set\n",
        "        test_loss, test_acc = validate(model, criterion, test_loader, device)\n",
        "        wandb.log({\"test_loss\": test_loss, \"test_acc\": test_acc})\n",
        "        print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "        # Return the trained model and test_loader for further analysis if needed\n",
        "        return model, test_loader\n"
      ],
      "metadata": {
        "id": "TTKIVf6xGFdb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define six hyperparameter configurations\n",
        "experiment_configs = [\n",
        "\n",
        "    {\"epochs\": 10, \"batch_size\": 128, \"learning_rate\": 0.001, \"optimizer\": \"adam\"},\n",
        "    {\"epochs\": 10, \"batch_size\": 128, \"learning_rate\": 0.5, \"optimizer\": \"adam\"},\n",
        "    {\"epochs\": 15, \"batch_size\": 128, \"learning_rate\": 0.0001, \"optimizer\": \"adam\"},\n",
        "    {\"epochs\": 10, \"batch_size\": 128, \"learning_rate\": 0.001, \"optimizer\": \"sgd\"},\n",
        "    {\"epochs\": 20, \"batch_size\": 128, \"learning_rate\": 0.0005, \"optimizer\": \"sgd\"},\n",
        "    {\"epochs\": 105, \"batch_size\": 128, \"learning_rate\": 0.1, \"optimizer\": \"sgd\", \"use_scheduler\": True}\n",
        "]\n",
        "\n",
        "# Run each experiment sequentially\n",
        "for config in experiment_configs:\n",
        "    print(\"\\nStarting experiment with configuration:\")\n",
        "    print(config)\n",
        "    run_experiment(config)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tjg4UvbXGL_m",
        "outputId": "ef0851b6-adf2-413f-9817-11d88f9b2641"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting experiment with configuration:\n",
            "{'epochs': 10, 'batch_size': 128, 'learning_rate': 0.001, 'optimizer': 'adam'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250202_051942-xeb29tvy</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication/runs/xeb29tvy' target=\"_blank\">restful-river-1</a></strong> to <a href='https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication' target=\"_blank\">https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication/runs/xeb29tvy' target=\"_blank\">https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication/runs/xeb29tvy</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 44.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Epoch [1/10] | Train Loss: 1.5607 | Train Acc: 0.4196 | Val Loss: 1.3287 | Val Acc: 0.5241\n",
            "Epoch [2/10] | Train Loss: 1.1281 | Train Acc: 0.5965 | Val Loss: 1.1918 | Val Acc: 0.5735\n",
            "Epoch [3/10] | Train Loss: 0.9390 | Train Acc: 0.6670 | Val Loss: 0.9930 | Val Acc: 0.6483\n",
            "Epoch [4/10] | Train Loss: 0.8159 | Train Acc: 0.7138 | Val Loss: 1.0505 | Val Acc: 0.6488\n",
            "Epoch [5/10] | Train Loss: 0.7365 | Train Acc: 0.7419 | Val Loss: 0.8432 | Val Acc: 0.7074\n",
            "Epoch [6/10] | Train Loss: 0.6712 | Train Acc: 0.7670 | Val Loss: 0.7309 | Val Acc: 0.7475\n",
            "Epoch [7/10] | Train Loss: 0.6288 | Train Acc: 0.7812 | Val Loss: 0.7598 | Val Acc: 0.7365\n",
            "Epoch [8/10] | Train Loss: 0.5941 | Train Acc: 0.7937 | Val Loss: 0.6918 | Val Acc: 0.7633\n",
            "Epoch [9/10] | Train Loss: 0.5747 | Train Acc: 0.8022 | Val Loss: 0.7910 | Val Acc: 0.7344\n",
            "Epoch [10/10] | Train Loss: 0.5402 | Train Acc: 0.8125 | Val Loss: 0.6486 | Val Acc: 0.7800\n",
            "Test Loss: 0.6388 | Test Acc: 0.7825\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▄▅▆▇▇▇███</td></tr><tr><td>train_loss</td><td>█▅▄▃▂▂▂▁▁▁</td></tr><tr><td>val_acc</td><td>▁▂▄▄▆▇▇█▇█</td></tr><tr><td>val_loss</td><td>█▇▅▅▃▂▂▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>learning_rate</td><td>0.001</td></tr><tr><td>optimizer</td><td>adam</td></tr><tr><td>test_acc</td><td>0.7825</td></tr><tr><td>test_loss</td><td>0.63882</td></tr><tr><td>train_acc</td><td>0.8125</td></tr><tr><td>train_loss</td><td>0.54017</td></tr><tr><td>val_acc</td><td>0.78</td></tr><tr><td>val_loss</td><td>0.6486</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">restful-river-1</strong> at: <a href='https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication/runs/xeb29tvy' target=\"_blank\">https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication/runs/xeb29tvy</a><br> View project at: <a href='https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication' target=\"_blank\">https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 18 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250202_051942-xeb29tvy/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting experiment with configuration:\n",
            "{'epochs': 10, 'batch_size': 128, 'learning_rate': 0.5, 'optimizer': 'adam'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250202_052429-cob23x6u</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication/runs/cob23x6u' target=\"_blank\">wobbly-snowflake-2</a></strong> to <a href='https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication' target=\"_blank\">https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication/runs/cob23x6u' target=\"_blank\">https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication/runs/cob23x6u</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch [1/10] | Train Loss: 2.7467 | Train Acc: 0.1236 | Val Loss: 13.9765 | Val Acc: 0.0773\n",
            "Epoch [2/10] | Train Loss: 2.4944 | Train Acc: 0.1225 | Val Loss: 2.3877 | Val Acc: 0.0963\n",
            "Epoch [3/10] | Train Loss: 2.3989 | Train Acc: 0.1147 | Val Loss: 2.3170 | Val Acc: 0.1156\n",
            "Epoch [4/10] | Train Loss: 2.4777 | Train Acc: 0.1135 | Val Loss: 2.3586 | Val Acc: 0.1025\n",
            "Epoch [5/10] | Train Loss: 2.3951 | Train Acc: 0.1066 | Val Loss: 2.3517 | Val Acc: 0.0641\n",
            "Epoch [6/10] | Train Loss: 2.3794 | Train Acc: 0.1075 | Val Loss: 2.4053 | Val Acc: 0.0995\n",
            "Epoch [7/10] | Train Loss: 2.5382 | Train Acc: 0.1058 | Val Loss: 38.1255 | Val Acc: 0.0995\n",
            "Epoch [8/10] | Train Loss: 2.3597 | Train Acc: 0.1151 | Val Loss: 54.7649 | Val Acc: 0.0995\n",
            "Epoch [9/10] | Train Loss: 2.5252 | Train Acc: 0.1039 | Val Loss: 197.2013 | Val Acc: 0.1006\n",
            "Epoch [10/10] | Train Loss: 2.5966 | Train Acc: 0.1114 | Val Loss: 2.3267 | Val Acc: 0.1023\n",
            "Test Loss: 2.3282 | Test Acc: 0.1000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_acc</td><td>██▅▄▂▂▂▅▁▄</td></tr><tr><td>train_loss</td><td>█▃▂▃▂▁▄▁▄▅</td></tr><tr><td>val_acc</td><td>▃▅█▆▁▆▆▆▆▆</td></tr><tr><td>val_loss</td><td>▁▁▁▁▁▁▂▃█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>learning_rate</td><td>0.5</td></tr><tr><td>optimizer</td><td>adam</td></tr><tr><td>test_acc</td><td>0.1</td></tr><tr><td>test_loss</td><td>2.32817</td></tr><tr><td>train_acc</td><td>0.11135</td></tr><tr><td>train_loss</td><td>2.59664</td></tr><tr><td>val_acc</td><td>0.1023</td></tr><tr><td>val_loss</td><td>2.32674</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">wobbly-snowflake-2</strong> at: <a href='https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication/runs/cob23x6u' target=\"_blank\">https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication/runs/cob23x6u</a><br> View project at: <a href='https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication' target=\"_blank\">https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 13 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250202_052429-cob23x6u/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting experiment with configuration:\n",
            "{'epochs': 15, 'batch_size': 128, 'learning_rate': 0.0001, 'optimizer': 'adam'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250202_052910-ama5bwy7</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication/runs/ama5bwy7' target=\"_blank\">rare-leaf-3</a></strong> to <a href='https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication' target=\"_blank\">https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication/runs/ama5bwy7' target=\"_blank\">https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication/runs/ama5bwy7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch [1/15] | Train Loss: 1.7831 | Train Acc: 0.3430 | Val Loss: 1.5681 | Val Acc: 0.4133\n",
            "Epoch [2/15] | Train Loss: 1.4445 | Train Acc: 0.4733 | Val Loss: 1.3780 | Val Acc: 0.4923\n",
            "Epoch [3/15] | Train Loss: 1.2796 | Train Acc: 0.5356 | Val Loss: 1.2704 | Val Acc: 0.5387\n",
            "Epoch [4/15] | Train Loss: 1.1639 | Train Acc: 0.5821 | Val Loss: 1.1740 | Val Acc: 0.5818\n",
            "Epoch [5/15] | Train Loss: 1.0741 | Train Acc: 0.6165 | Val Loss: 1.1268 | Val Acc: 0.5934\n",
            "Epoch [6/15] | Train Loss: 1.0006 | Train Acc: 0.6453 | Val Loss: 1.0207 | Val Acc: 0.6367\n",
            "Epoch [7/15] | Train Loss: 0.9428 | Train Acc: 0.6623 | Val Loss: 0.9918 | Val Acc: 0.6450\n",
            "Epoch [8/15] | Train Loss: 0.8908 | Train Acc: 0.6841 | Val Loss: 0.9442 | Val Acc: 0.6616\n",
            "Epoch [9/15] | Train Loss: 0.8510 | Train Acc: 0.7001 | Val Loss: 0.9360 | Val Acc: 0.6732\n",
            "Epoch [10/15] | Train Loss: 0.8108 | Train Acc: 0.7134 | Val Loss: 0.8827 | Val Acc: 0.6913\n",
            "Epoch [11/15] | Train Loss: 0.7779 | Train Acc: 0.7279 | Val Loss: 0.8219 | Val Acc: 0.7119\n",
            "Epoch [12/15] | Train Loss: 0.7439 | Train Acc: 0.7394 | Val Loss: 0.8290 | Val Acc: 0.7129\n",
            "Epoch [13/15] | Train Loss: 0.7120 | Train Acc: 0.7492 | Val Loss: 0.8152 | Val Acc: 0.7160\n",
            "Epoch [14/15] | Train Loss: 0.6898 | Train Acc: 0.7585 | Val Loss: 0.7833 | Val Acc: 0.7290\n",
            "Epoch [15/15] | Train Loss: 0.6702 | Train Acc: 0.7675 | Val Loss: 0.7570 | Val Acc: 0.7381\n",
            "Test Loss: 0.7612 | Test Acc: 0.7435\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▃▄▅▆▆▆▇▇▇▇████</td></tr><tr><td>train_loss</td><td>█▆▅▄▄▃▃▂▂▂▂▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▃▄▅▅▆▆▆▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▆▅▅▄▃▃▃▃▂▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>optimizer</td><td>adam</td></tr><tr><td>test_acc</td><td>0.7435</td></tr><tr><td>test_loss</td><td>0.76124</td></tr><tr><td>train_acc</td><td>0.76755</td></tr><tr><td>train_loss</td><td>0.67019</td></tr><tr><td>val_acc</td><td>0.7381</td></tr><tr><td>val_loss</td><td>0.75698</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">rare-leaf-3</strong> at: <a href='https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication/runs/ama5bwy7' target=\"_blank\">https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication/runs/ama5bwy7</a><br> View project at: <a href='https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication' target=\"_blank\">https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 30 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250202_052910-ama5bwy7/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting experiment with configuration:\n",
            "{'epochs': 10, 'batch_size': 128, 'learning_rate': 0.001, 'optimizer': 'sgd'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250202_053606-0d2d4cp2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication/runs/0d2d4cp2' target=\"_blank\">gallant-deluge-4</a></strong> to <a href='https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication' target=\"_blank\">https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication/runs/0d2d4cp2' target=\"_blank\">https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication/runs/0d2d4cp2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch [1/10] | Train Loss: 1.8860 | Train Acc: 0.2881 | Val Loss: 1.6320 | Val Acc: 0.3818\n",
            "Epoch [2/10] | Train Loss: 1.5239 | Train Acc: 0.4301 | Val Loss: 1.4967 | Val Acc: 0.4501\n",
            "Epoch [3/10] | Train Loss: 1.3725 | Train Acc: 0.4953 | Val Loss: 1.3748 | Val Acc: 0.5082\n",
            "Epoch [4/10] | Train Loss: 1.2427 | Train Acc: 0.5482 | Val Loss: 1.2251 | Val Acc: 0.5586\n",
            "Epoch [5/10] | Train Loss: 1.1248 | Train Acc: 0.5962 | Val Loss: 1.1179 | Val Acc: 0.6009\n",
            "Epoch [6/10] | Train Loss: 1.0297 | Train Acc: 0.6330 | Val Loss: 1.0518 | Val Acc: 0.6277\n",
            "Epoch [7/10] | Train Loss: 0.9576 | Train Acc: 0.6574 | Val Loss: 0.9563 | Val Acc: 0.6633\n",
            "Epoch [8/10] | Train Loss: 0.8885 | Train Acc: 0.6841 | Val Loss: 0.9854 | Val Acc: 0.6548\n",
            "Epoch [9/10] | Train Loss: 0.8413 | Train Acc: 0.7038 | Val Loss: 0.8415 | Val Acc: 0.6992\n",
            "Epoch [10/10] | Train Loss: 0.7926 | Train Acc: 0.7200 | Val Loss: 0.8643 | Val Acc: 0.6997\n",
            "Test Loss: 0.8822 | Test Acc: 0.7022\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▃▄▅▆▇▇▇██</td></tr><tr><td>train_loss</td><td>█▆▅▄▃▃▂▂▁▁</td></tr><tr><td>val_acc</td><td>▁▃▄▅▆▆▇▇██</td></tr><tr><td>val_loss</td><td>█▇▆▄▃▃▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>learning_rate</td><td>0.001</td></tr><tr><td>optimizer</td><td>sgd</td></tr><tr><td>test_acc</td><td>0.7022</td></tr><tr><td>test_loss</td><td>0.88218</td></tr><tr><td>train_acc</td><td>0.72</td></tr><tr><td>train_loss</td><td>0.79265</td></tr><tr><td>val_acc</td><td>0.6997</td></tr><tr><td>val_loss</td><td>0.86432</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">gallant-deluge-4</strong> at: <a href='https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication/runs/0d2d4cp2' target=\"_blank\">https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication/runs/0d2d4cp2</a><br> View project at: <a href='https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication' target=\"_blank\">https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 19 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250202_053606-0d2d4cp2/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting experiment with configuration:\n",
            "{'epochs': 20, 'batch_size': 128, 'learning_rate': 0.0005, 'optimizer': 'sgd'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250202_054044-nv38xwho</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication/runs/nv38xwho' target=\"_blank\">sage-brook-5</a></strong> to <a href='https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication' target=\"_blank\">https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication/runs/nv38xwho' target=\"_blank\">https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication/runs/nv38xwho</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch [1/20] | Train Loss: 1.9825 | Train Acc: 0.2519 | Val Loss: 1.7715 | Val Acc: 0.3221\n",
            "Epoch [2/20] | Train Loss: 1.6530 | Train Acc: 0.3734 | Val Loss: 1.5896 | Val Acc: 0.3942\n",
            "Epoch [3/20] | Train Loss: 1.5013 | Train Acc: 0.4405 | Val Loss: 1.4468 | Val Acc: 0.4643\n",
            "Epoch [4/20] | Train Loss: 1.3795 | Train Acc: 0.4901 | Val Loss: 1.4054 | Val Acc: 0.4885\n",
            "Epoch [5/20] | Train Loss: 1.2935 | Train Acc: 0.5258 | Val Loss: 1.3021 | Val Acc: 0.5268\n",
            "Epoch [6/20] | Train Loss: 1.2213 | Train Acc: 0.5541 | Val Loss: 1.2355 | Val Acc: 0.5511\n",
            "Epoch [7/20] | Train Loss: 1.1632 | Train Acc: 0.5786 | Val Loss: 1.1261 | Val Acc: 0.5913\n",
            "Epoch [8/20] | Train Loss: 1.1084 | Train Acc: 0.5994 | Val Loss: 1.1085 | Val Acc: 0.6000\n",
            "Epoch [9/20] | Train Loss: 1.0574 | Train Acc: 0.6197 | Val Loss: 1.0578 | Val Acc: 0.6151\n",
            "Epoch [10/20] | Train Loss: 1.0183 | Train Acc: 0.6338 | Val Loss: 1.0684 | Val Acc: 0.6129\n",
            "Epoch [11/20] | Train Loss: 0.9836 | Train Acc: 0.6492 | Val Loss: 0.9984 | Val Acc: 0.6424\n",
            "Epoch [12/20] | Train Loss: 0.9517 | Train Acc: 0.6585 | Val Loss: 0.9541 | Val Acc: 0.6576\n",
            "Epoch [13/20] | Train Loss: 0.9180 | Train Acc: 0.6703 | Val Loss: 0.9663 | Val Acc: 0.6546\n",
            "Epoch [14/20] | Train Loss: 0.8936 | Train Acc: 0.6809 | Val Loss: 0.9006 | Val Acc: 0.6803\n",
            "Epoch [15/20] | Train Loss: 0.8663 | Train Acc: 0.6924 | Val Loss: 0.8917 | Val Acc: 0.6804\n",
            "Epoch [16/20] | Train Loss: 0.8409 | Train Acc: 0.7022 | Val Loss: 0.8965 | Val Acc: 0.6821\n",
            "Epoch [17/20] | Train Loss: 0.8186 | Train Acc: 0.7096 | Val Loss: 0.8942 | Val Acc: 0.6828\n",
            "Epoch [18/20] | Train Loss: 0.8000 | Train Acc: 0.7158 | Val Loss: 0.9028 | Val Acc: 0.6778\n",
            "Epoch [19/20] | Train Loss: 0.7758 | Train Acc: 0.7242 | Val Loss: 0.8282 | Val Acc: 0.7040\n",
            "Epoch [20/20] | Train Loss: 0.7524 | Train Acc: 0.7318 | Val Loss: 0.8006 | Val Acc: 0.7180\n",
            "Test Loss: 0.8184 | Test Acc: 0.7114\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▃▄▄▅▅▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train_loss</td><td>█▆▅▅▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▂▄▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇██</td></tr><tr><td>val_loss</td><td>█▇▆▅▅▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>learning_rate</td><td>0.0005</td></tr><tr><td>optimizer</td><td>sgd</td></tr><tr><td>test_acc</td><td>0.7114</td></tr><tr><td>test_loss</td><td>0.81842</td></tr><tr><td>train_acc</td><td>0.7318</td></tr><tr><td>train_loss</td><td>0.75241</td></tr><tr><td>val_acc</td><td>0.718</td></tr><tr><td>val_loss</td><td>0.80063</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">sage-brook-5</strong> at: <a href='https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication/runs/nv38xwho' target=\"_blank\">https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication/runs/nv38xwho</a><br> View project at: <a href='https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication' target=\"_blank\">https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 37 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250202_054044-nv38xwho/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting experiment with configuration:\n",
            "{'epochs': 105, 'batch_size': 128, 'learning_rate': 0.1, 'optimizer': 'sgd', 'use_scheduler': True}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250202_054954-wgxixpiq</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication/runs/wgxixpiq' target=\"_blank\">lucky-lion-6</a></strong> to <a href='https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication' target=\"_blank\">https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication/runs/wgxixpiq' target=\"_blank\">https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication/runs/wgxixpiq</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch [1/105] | Train Loss: 1.7345 | Train Acc: 0.3479 | Val Loss: 1.4564 | Val Acc: 0.4609\n",
            "Epoch [2/105] | Train Loss: 1.3230 | Train Acc: 0.5144 | Val Loss: 1.2047 | Val Acc: 0.5743\n",
            "Epoch [3/105] | Train Loss: 1.0784 | Train Acc: 0.6136 | Val Loss: 1.1107 | Val Acc: 0.6126\n",
            "Epoch [4/105] | Train Loss: 0.9124 | Train Acc: 0.6769 | Val Loss: 1.0348 | Val Acc: 0.6317\n",
            "Epoch [5/105] | Train Loss: 0.8011 | Train Acc: 0.7207 | Val Loss: 1.0380 | Val Acc: 0.6400\n",
            "Epoch [6/105] | Train Loss: 0.7300 | Train Acc: 0.7458 | Val Loss: 0.8585 | Val Acc: 0.7102\n",
            "Epoch [7/105] | Train Loss: 0.6760 | Train Acc: 0.7659 | Val Loss: 0.8318 | Val Acc: 0.7099\n",
            "Epoch [8/105] | Train Loss: 0.6450 | Train Acc: 0.7783 | Val Loss: 1.0066 | Val Acc: 0.6795\n",
            "Epoch [9/105] | Train Loss: 0.6191 | Train Acc: 0.7875 | Val Loss: 0.7914 | Val Acc: 0.7306\n",
            "Epoch [10/105] | Train Loss: 0.5966 | Train Acc: 0.7938 | Val Loss: 0.8744 | Val Acc: 0.6986\n",
            "Epoch [11/105] | Train Loss: 0.5824 | Train Acc: 0.7982 | Val Loss: 0.8869 | Val Acc: 0.7046\n",
            "Epoch [12/105] | Train Loss: 0.5631 | Train Acc: 0.8063 | Val Loss: 0.7554 | Val Acc: 0.7439\n",
            "Epoch [13/105] | Train Loss: 0.5526 | Train Acc: 0.8074 | Val Loss: 0.7946 | Val Acc: 0.7320\n",
            "Epoch [14/105] | Train Loss: 0.5351 | Train Acc: 0.8139 | Val Loss: 0.7585 | Val Acc: 0.7453\n",
            "Epoch [15/105] | Train Loss: 0.5261 | Train Acc: 0.8186 | Val Loss: 0.7653 | Val Acc: 0.7371\n",
            "Epoch [16/105] | Train Loss: 0.5171 | Train Acc: 0.8224 | Val Loss: 0.8524 | Val Acc: 0.7098\n",
            "Epoch [17/105] | Train Loss: 0.5169 | Train Acc: 0.8219 | Val Loss: 0.6734 | Val Acc: 0.7716\n",
            "Epoch [18/105] | Train Loss: 0.5069 | Train Acc: 0.8254 | Val Loss: 0.8067 | Val Acc: 0.7429\n",
            "Epoch [19/105] | Train Loss: 0.5003 | Train Acc: 0.8269 | Val Loss: 0.6728 | Val Acc: 0.7764\n",
            "Epoch [20/105] | Train Loss: 0.4904 | Train Acc: 0.8305 | Val Loss: 0.7218 | Val Acc: 0.7589\n",
            "Epoch [21/105] | Train Loss: 0.4908 | Train Acc: 0.8312 | Val Loss: 0.7444 | Val Acc: 0.7534\n",
            "Epoch [22/105] | Train Loss: 0.4877 | Train Acc: 0.8352 | Val Loss: 0.7516 | Val Acc: 0.7538\n",
            "Epoch [23/105] | Train Loss: 0.4800 | Train Acc: 0.8347 | Val Loss: 0.5740 | Val Acc: 0.8063\n",
            "Epoch [24/105] | Train Loss: 0.4741 | Train Acc: 0.8366 | Val Loss: 0.7157 | Val Acc: 0.7556\n",
            "Epoch [25/105] | Train Loss: 0.4733 | Train Acc: 0.8379 | Val Loss: 0.7671 | Val Acc: 0.7539\n",
            "Epoch [26/105] | Train Loss: 0.4675 | Train Acc: 0.8399 | Val Loss: 0.5988 | Val Acc: 0.7928\n",
            "Epoch [27/105] | Train Loss: 0.4644 | Train Acc: 0.8411 | Val Loss: 0.7068 | Val Acc: 0.7673\n",
            "Epoch [28/105] | Train Loss: 0.4601 | Train Acc: 0.8429 | Val Loss: 0.7057 | Val Acc: 0.7678\n",
            "Epoch [29/105] | Train Loss: 0.4600 | Train Acc: 0.8427 | Val Loss: 0.6699 | Val Acc: 0.7774\n",
            "Epoch [30/105] | Train Loss: 0.4583 | Train Acc: 0.8409 | Val Loss: 0.9980 | Val Acc: 0.6745\n",
            "Epoch [31/105] | Train Loss: 0.4529 | Train Acc: 0.8451 | Val Loss: 0.5326 | Val Acc: 0.8132\n",
            "Epoch [32/105] | Train Loss: 0.4486 | Train Acc: 0.8458 | Val Loss: 0.5620 | Val Acc: 0.8103\n",
            "Epoch [33/105] | Train Loss: 0.4514 | Train Acc: 0.8437 | Val Loss: 0.6181 | Val Acc: 0.7870\n",
            "Epoch [34/105] | Train Loss: 0.4430 | Train Acc: 0.8458 | Val Loss: 0.6926 | Val Acc: 0.7698\n",
            "Epoch [35/105] | Train Loss: 0.4398 | Train Acc: 0.8495 | Val Loss: 0.6352 | Val Acc: 0.7836\n",
            "Epoch [36/105] | Train Loss: 0.4370 | Train Acc: 0.8505 | Val Loss: 0.6959 | Val Acc: 0.7740\n",
            "Epoch [37/105] | Train Loss: 0.4339 | Train Acc: 0.8526 | Val Loss: 0.6237 | Val Acc: 0.7894\n",
            "Epoch [38/105] | Train Loss: 0.4459 | Train Acc: 0.8458 | Val Loss: 0.5671 | Val Acc: 0.8081\n",
            "Epoch [39/105] | Train Loss: 0.4369 | Train Acc: 0.8494 | Val Loss: 0.7205 | Val Acc: 0.7571\n",
            "Epoch [40/105] | Train Loss: 0.4284 | Train Acc: 0.8526 | Val Loss: 0.6649 | Val Acc: 0.7820\n",
            "Epoch [41/105] | Train Loss: 0.4261 | Train Acc: 0.8533 | Val Loss: 0.5438 | Val Acc: 0.8136\n",
            "Epoch [42/105] | Train Loss: 0.4297 | Train Acc: 0.8523 | Val Loss: 0.8361 | Val Acc: 0.7272\n",
            "Epoch [43/105] | Train Loss: 0.4336 | Train Acc: 0.8514 | Val Loss: 0.9090 | Val Acc: 0.7252\n",
            "Epoch [44/105] | Train Loss: 0.4274 | Train Acc: 0.8518 | Val Loss: 0.7515 | Val Acc: 0.7653\n",
            "Epoch [45/105] | Train Loss: 0.4244 | Train Acc: 0.8562 | Val Loss: 0.5965 | Val Acc: 0.7970\n",
            "Epoch [46/105] | Train Loss: 0.4248 | Train Acc: 0.8549 | Val Loss: 0.5825 | Val Acc: 0.8013\n",
            "Epoch [47/105] | Train Loss: 0.4209 | Train Acc: 0.8558 | Val Loss: 0.6046 | Val Acc: 0.7963\n",
            "Epoch [48/105] | Train Loss: 0.4195 | Train Acc: 0.8544 | Val Loss: 0.5941 | Val Acc: 0.8006\n",
            "Epoch [49/105] | Train Loss: 0.4236 | Train Acc: 0.8550 | Val Loss: 0.4890 | Val Acc: 0.8329\n",
            "Epoch [50/105] | Train Loss: 0.4126 | Train Acc: 0.8589 | Val Loss: 0.5530 | Val Acc: 0.8105\n",
            "Epoch [51/105] | Train Loss: 0.4157 | Train Acc: 0.8566 | Val Loss: 0.5455 | Val Acc: 0.8179\n",
            "Epoch [52/105] | Train Loss: 0.4122 | Train Acc: 0.8597 | Val Loss: 0.8105 | Val Acc: 0.7435\n",
            "Epoch [53/105] | Train Loss: 0.2650 | Train Acc: 0.9110 | Val Loss: 0.2973 | Val Acc: 0.8989\n",
            "Epoch [54/105] | Train Loss: 0.2148 | Train Acc: 0.9266 | Val Loss: 0.2856 | Val Acc: 0.9040\n",
            "Epoch [55/105] | Train Loss: 0.1938 | Train Acc: 0.9338 | Val Loss: 0.2743 | Val Acc: 0.9065\n",
            "Epoch [56/105] | Train Loss: 0.1838 | Train Acc: 0.9375 | Val Loss: 0.2716 | Val Acc: 0.9093\n",
            "Epoch [57/105] | Train Loss: 0.1708 | Train Acc: 0.9434 | Val Loss: 0.2689 | Val Acc: 0.9082\n",
            "Epoch [58/105] | Train Loss: 0.1597 | Train Acc: 0.9463 | Val Loss: 0.2670 | Val Acc: 0.9129\n",
            "Epoch [59/105] | Train Loss: 0.1534 | Train Acc: 0.9477 | Val Loss: 0.2705 | Val Acc: 0.9107\n",
            "Epoch [60/105] | Train Loss: 0.1454 | Train Acc: 0.9520 | Val Loss: 0.2686 | Val Acc: 0.9131\n",
            "Epoch [61/105] | Train Loss: 0.1394 | Train Acc: 0.9519 | Val Loss: 0.2696 | Val Acc: 0.9112\n",
            "Epoch [62/105] | Train Loss: 0.1327 | Train Acc: 0.9556 | Val Loss: 0.2860 | Val Acc: 0.9075\n",
            "Epoch [63/105] | Train Loss: 0.1325 | Train Acc: 0.9541 | Val Loss: 0.2796 | Val Acc: 0.9074\n",
            "Epoch [64/105] | Train Loss: 0.1237 | Train Acc: 0.9579 | Val Loss: 0.2832 | Val Acc: 0.9053\n",
            "Epoch [65/105] | Train Loss: 0.1199 | Train Acc: 0.9596 | Val Loss: 0.2724 | Val Acc: 0.9098\n",
            "Epoch [66/105] | Train Loss: 0.1195 | Train Acc: 0.9592 | Val Loss: 0.2940 | Val Acc: 0.9061\n",
            "Epoch [67/105] | Train Loss: 0.1129 | Train Acc: 0.9630 | Val Loss: 0.2799 | Val Acc: 0.9089\n",
            "Epoch [68/105] | Train Loss: 0.1099 | Train Acc: 0.9630 | Val Loss: 0.2820 | Val Acc: 0.9079\n",
            "Epoch [69/105] | Train Loss: 0.1111 | Train Acc: 0.9618 | Val Loss: 0.2904 | Val Acc: 0.9067\n",
            "Epoch [70/105] | Train Loss: 0.1072 | Train Acc: 0.9637 | Val Loss: 0.2942 | Val Acc: 0.9085\n",
            "Epoch [71/105] | Train Loss: 0.1034 | Train Acc: 0.9649 | Val Loss: 0.3048 | Val Acc: 0.9039\n",
            "Epoch [72/105] | Train Loss: 0.1040 | Train Acc: 0.9648 | Val Loss: 0.2804 | Val Acc: 0.9129\n",
            "Epoch [73/105] | Train Loss: 0.1005 | Train Acc: 0.9658 | Val Loss: 0.2951 | Val Acc: 0.9073\n",
            "Epoch [74/105] | Train Loss: 0.1023 | Train Acc: 0.9645 | Val Loss: 0.3167 | Val Acc: 0.9026\n",
            "Epoch [75/105] | Train Loss: 0.0945 | Train Acc: 0.9684 | Val Loss: 0.3005 | Val Acc: 0.9040\n",
            "Epoch [76/105] | Train Loss: 0.0986 | Train Acc: 0.9661 | Val Loss: 0.3110 | Val Acc: 0.9044\n",
            "Epoch [77/105] | Train Loss: 0.0987 | Train Acc: 0.9662 | Val Loss: 0.3439 | Val Acc: 0.8976\n",
            "Epoch [78/105] | Train Loss: 0.0956 | Train Acc: 0.9686 | Val Loss: 0.3031 | Val Acc: 0.9047\n",
            "Epoch [79/105] | Train Loss: 0.0658 | Train Acc: 0.9792 | Val Loss: 0.2631 | Val Acc: 0.9199\n",
            "Epoch [80/105] | Train Loss: 0.0542 | Train Acc: 0.9844 | Val Loss: 0.2603 | Val Acc: 0.9207\n",
            "Epoch [81/105] | Train Loss: 0.0498 | Train Acc: 0.9852 | Val Loss: 0.2655 | Val Acc: 0.9203\n",
            "Epoch [82/105] | Train Loss: 0.0454 | Train Acc: 0.9873 | Val Loss: 0.2748 | Val Acc: 0.9186\n",
            "Epoch [83/105] | Train Loss: 0.0439 | Train Acc: 0.9878 | Val Loss: 0.2684 | Val Acc: 0.9174\n",
            "Epoch [84/105] | Train Loss: 0.0409 | Train Acc: 0.9887 | Val Loss: 0.2629 | Val Acc: 0.9213\n",
            "Epoch [85/105] | Train Loss: 0.0388 | Train Acc: 0.9896 | Val Loss: 0.2690 | Val Acc: 0.9172\n",
            "Epoch [86/105] | Train Loss: 0.0375 | Train Acc: 0.9901 | Val Loss: 0.2726 | Val Acc: 0.9196\n",
            "Epoch [87/105] | Train Loss: 0.0366 | Train Acc: 0.9898 | Val Loss: 0.2732 | Val Acc: 0.9200\n",
            "Epoch [88/105] | Train Loss: 0.0360 | Train Acc: 0.9905 | Val Loss: 0.2730 | Val Acc: 0.9193\n",
            "Epoch [89/105] | Train Loss: 0.0340 | Train Acc: 0.9910 | Val Loss: 0.2729 | Val Acc: 0.9206\n",
            "Epoch [90/105] | Train Loss: 0.0326 | Train Acc: 0.9915 | Val Loss: 0.2776 | Val Acc: 0.9187\n",
            "Epoch [91/105] | Train Loss: 0.0327 | Train Acc: 0.9911 | Val Loss: 0.2800 | Val Acc: 0.9187\n",
            "Epoch [92/105] | Train Loss: 0.0309 | Train Acc: 0.9918 | Val Loss: 0.2770 | Val Acc: 0.9193\n",
            "Epoch [93/105] | Train Loss: 0.0301 | Train Acc: 0.9928 | Val Loss: 0.2757 | Val Acc: 0.9212\n",
            "Epoch [94/105] | Train Loss: 0.0309 | Train Acc: 0.9923 | Val Loss: 0.2840 | Val Acc: 0.9171\n",
            "Epoch [95/105] | Train Loss: 0.0304 | Train Acc: 0.9921 | Val Loss: 0.2811 | Val Acc: 0.9209\n",
            "Epoch [96/105] | Train Loss: 0.0289 | Train Acc: 0.9928 | Val Loss: 0.2782 | Val Acc: 0.9211\n",
            "Epoch [97/105] | Train Loss: 0.0290 | Train Acc: 0.9927 | Val Loss: 0.2770 | Val Acc: 0.9207\n",
            "Epoch [98/105] | Train Loss: 0.0279 | Train Acc: 0.9929 | Val Loss: 0.2836 | Val Acc: 0.9183\n",
            "Epoch [99/105] | Train Loss: 0.0269 | Train Acc: 0.9935 | Val Loss: 0.2794 | Val Acc: 0.9189\n",
            "Epoch [100/105] | Train Loss: 0.0259 | Train Acc: 0.9937 | Val Loss: 0.2894 | Val Acc: 0.9190\n",
            "Epoch [101/105] | Train Loss: 0.0261 | Train Acc: 0.9936 | Val Loss: 0.2766 | Val Acc: 0.9207\n",
            "Epoch [102/105] | Train Loss: 0.0254 | Train Acc: 0.9941 | Val Loss: 0.2965 | Val Acc: 0.9175\n",
            "Epoch [103/105] | Train Loss: 0.0253 | Train Acc: 0.9937 | Val Loss: 0.2832 | Val Acc: 0.9231\n",
            "Epoch [104/105] | Train Loss: 0.0246 | Train Acc: 0.9943 | Val Loss: 0.2976 | Val Acc: 0.9163\n",
            "Epoch [105/105] | Train Loss: 0.0246 | Train Acc: 0.9938 | Val Loss: 0.2923 | Val Acc: 0.9163\n",
            "Test Loss: 0.3020 | Test Acc: 0.9202\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▆▆▆▇▇▇████</td></tr><tr><td>learning_rate</td><td>█████████████████████▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇██████████████████</td></tr><tr><td>train_loss</td><td>█▇▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▃▃▃▃▅▄▄▄▅▅▅▅▆▄▅████████████████████████</td></tr><tr><td>val_loss</td><td>█▇▅▆▅▄▅█▄▄▄▄▅▅▆▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>105</td></tr><tr><td>learning_rate</td><td>0.001</td></tr><tr><td>optimizer</td><td>sgd</td></tr><tr><td>test_acc</td><td>0.9202</td></tr><tr><td>test_loss</td><td>0.30197</td></tr><tr><td>train_acc</td><td>0.99382</td></tr><tr><td>train_loss</td><td>0.02462</td></tr><tr><td>val_acc</td><td>0.9163</td></tr><tr><td>val_loss</td><td>0.29232</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lucky-lion-6</strong> at: <a href='https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication/runs/wgxixpiq' target=\"_blank\">https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication/runs/wgxixpiq</a><br> View project at: <a href='https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication' target=\"_blank\">https://wandb.ai/g22168559-vellore-institute-of-technology/cifar10_classification_Data_science_assignment_Tata_communication</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 130 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250202_054954-wgxixpiq/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Initialize the model architecture (must match the saved model's architecture)\n",
        "model = get_resnet32_for_cifar10().to(device)\n",
        "\n",
        "\n",
        "model.load_state_dict(torch.load(\"/content/wgxixpiq_best_epoch_103_1738478116.pth\", map_location=device))\n",
        "model.eval()  # Set to evaluation mode\n",
        "\n",
        "print(\"Model successfully loaded and ready for predictions!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Uzn1wH-dDNH",
        "outputId": "b2c48159-5602-4811-86a6-815652612142"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model successfully loaded and ready for predictions!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-38e180de7191>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"/content/wgxixpiq_best_epoch_103_1738478116.pth\", map_location=device))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "])\n",
        "\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "print(\" Test dataset loaded successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4EGfYVqddLi",
        "outputId": "0eb19ab5-caf2-45eb-ad92-fe7374bd92ca"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:08<00:00, 21.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            " Test dataset loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  CIFAR-10 class names\n",
        "cifar10_class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "                        'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# Run the prediction function\n",
        "visualize_predictions(model, test_loader, cifar10_class_names, num_images=5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "QyCto9dwdfk3",
        "outputId": "b5e18f32-b507-4a8f-dbad-e70660ed27aa"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x300 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis and Insights Report\n",
        "\n",
        "## 1. Overview\n",
        "This report analyzes the **CIFAR-10 classification task using ResNet-32**, trained across multiple experiments with different hyperparameters. The experiments were logged in **Weights & Biases (W&B)**, allowing a detailed comparison of training progress, model performance, and optimization strategies.\n",
        "\n",
        "### Key Performance Metrics:\n",
        "| Run Name          | Optimizer | Learning Rate | Epochs | Test Accuracy | Train Accuracy | Validation Accuracy |\n",
        "|------------------|-----------|--------------|--------|--------------|--------------|-------------------|\n",
        "| lucky-lion-6     | SGD       | 0.1          | 105    | 92.02%       | 99.38%       | 91.63%            |\n",
        "| sage-brook-5     | SGD       | 0.0005       | 20     | 71.14%       | 75.24%       | 71.80%            |\n",
        "| gallant-deluge-4 | SGD       | 0.001        | 10     | 70.22%       | 79.26%       | 69.97%            |\n",
        "| rare-leaf-3      | Adam      | 0.0001       | 15     | 74.35%       | 76.75%       | 73.81%            |\n",
        "| wobbly-snowflake-2 | Adam    | 0.5          | 10     | 10.23%       | 11.13%       | 10.23%            |\n",
        "| restful-river-1  | Adam      | 0.001        | 10     | 78.25%       | 81.25%       | 78.00%            |\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Experiment Comparisons Using W&B Dashboards\n",
        "### Key Observations from W&B Graphs:\n",
        "- Higher learning rates (0.5) led to unstable training, as seen in `wobbly-snowflake-2` with only 10% accuracy.\n",
        "- SGD with LR scheduling (`lucky-lion-6`) showed the best performance with a final test accuracy of **92.02%**.\n",
        "- Adam optimizer resulted in smooth but slightly lower convergence, peaking at **78.25% accuracy** in `restful-river-1`.\n",
        "- The model required at least 100 epochs for optimal performance; shorter runs underperformed.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. How Hyperparameters Affected Performance\n",
        "\n",
        "### Learning Rate (LR) Impact:\n",
        "- Best LR: **0.1 (with decay at epochs 50 & 75)** → Faster convergence, optimal test accuracy.\n",
        "- Worst LR: **0.5** → Highly unstable training, poor generalization (Test Accuracy = **10.23%**).\n",
        "\n",
        "### Optimizer Comparison: SGD vs. Adam\n",
        "- SGD (with momentum) outperformed Adam over long runs.\n",
        "- Adam performed well for quick convergence but plateaued at ~78% accuracy.\n",
        "\n",
        "### Effect of Training Epochs:\n",
        "- **10-20 epochs were insufficient** for high accuracy (>90%).\n",
        "- **100+ epochs improved generalization**, especially for SGD.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Best Experiment & Why?\n",
        "### Best Performing Run: `lucky-lion-6`\n",
        "- **Optimizer:** SGD (Momentum = 0.9)\n",
        "- **Learning Rate:** 0.1 (decayed at 50 & 75 epochs)\n",
        "- **Epochs:** 105\n",
        "- **Test Accuracy:** 92.02%\n",
        "- **Validation Accuracy:** 91.63%\n",
        "\n",
        "### Why was it the best?\n",
        "- Effective Learning Rate Scheduling: Allowed controlled optimization.\n",
        "- SGD with momentum stabilized gradients, reducing noise.\n",
        "- Longer training (105 epochs) provided optimal feature learning.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Summary & Final Insights\n",
        "- **SGD with momentum outperforms Adam in longer runs.**  \n",
        "- **High learning rates without scheduling result in poor performance.**  \n",
        "- **Longer training (100+ epochs) is crucial for optimal accuracy.**  \n",
        "- **Best accuracy achieved = 92.02% using SGD + LR scheduler (`lucky-lion-6`).**  \n",
        "\n",
        "This experiment successfully optimized CIFAR-10 classification using ResNet-32 and W&B for tracking.\n"
      ],
      "metadata": {
        "id": "ooRV5PhO14Nz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Mhbyb3xdvQu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}