{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Deep Learning Experiments on CIFAR-10\n",
        "\n",
        "\n",
        "*   Name - Gaurav Sharan\n",
        "*   Registration Number - 20MIA1081\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "XrbdxnuxJhhA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2oiFfGyJQHc",
        "outputId": "b0500b88-6a66-48d1-bf96-ab0e6a8a8b2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.25.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.20.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision wandb\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F  # For activation functions\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import wandb\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Log in to Weights & Biases\n",
        "wandb.login()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "N1qP266kJstM",
        "outputId": "f8ce4e89-d5fe-4ef3-acca-7b2d70ac21ec"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgauravengineer85\u001b[0m (\u001b[33mgauravengineer85-vellore-institute-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1  # For basic block the expansion factor is 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        \"\"\"\n",
        "        A basic residual block for ResNet‑32.\n",
        "        Args:\n",
        "          in_planes: Number of input channels.\n",
        "          planes: Number of output channels.\n",
        "          stride: Stride for the first convolution.\n",
        "        \"\"\"\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        # Define the shortcut connection\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet32(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        \"\"\"\n",
        "        Constructs a ResNet‑32 model for CIFAR‑10.\n",
        "        Args:\n",
        "          block: Block type (BasicBlock).\n",
        "          num_blocks: A list specifying how many blocks each layer should have (for ResNet-32, this would be [5, 5, 5]).\n",
        "          num_classes: Number of output classes (10 for CIFAR‑10).\n",
        "        \"\"\"\n",
        "        super(ResNet32, self).__init__()\n",
        "        self.in_planes = 16\n",
        "\n",
        "        # Initial convolution layer (3×3 conv for CIFAR‑10)\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "\n",
        "        # Creating three layers (groups) of residual blocks\n",
        "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
        "\n",
        "        # Global average pooling and final fully connected layer\n",
        "        self.linear = nn.Linear(64, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        \"\"\"\n",
        "        Creates one group (layer) of residual blocks.\n",
        "        \"\"\"\n",
        "        strides = [stride] + [1]*(num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        # Global average pooling over spatial dimensions\n",
        "        out = F.avg_pool2d(out, out.size(3))\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def get_resnet32_for_cifar10():\n",
        "    \"\"\"\n",
        "    Instantiates a ResNet‑32 model for CIFAR‑10.\n",
        "    For ResNet‑32, we use 5 residual blocks per layer: [5, 5, 5].\n",
        "    Returns:\n",
        "      model (nn.Module): A ResNet‑32 model.\n",
        "    \"\"\"\n",
        "    return ResNet32(BasicBlock, [5, 5, 5], num_classes=10)\n"
      ],
      "metadata": {
        "id": "cNjk0TZGL9yj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, optimizer, criterion, train_loader, device):\n",
        "    \"\"\"\n",
        "    Trains the model for one epoch.\n",
        "\n",
        "    Args:\n",
        "      model (nn.Module): Model to train.\n",
        "      optimizer: Optimizer.\n",
        "      criterion: Loss function.\n",
        "      train_loader (DataLoader): Training data loader.\n",
        "      device: CPU or GPU.\n",
        "\n",
        "    Returns:\n",
        "      epoch_loss (float): Average training loss.\n",
        "      epoch_acc (float): Training accuracy.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def validate(model, criterion, loader, device):\n",
        "    \"\"\"\n",
        "    Evaluates the model on a validation/test set.\n",
        "\n",
        "    Args:\n",
        "      model (nn.Module): Model to evaluate.\n",
        "      criterion: Loss function.\n",
        "      loader (DataLoader): Validation/test data loader.\n",
        "      device: CPU or GPU.\n",
        "\n",
        "    Returns:\n",
        "      epoch_loss (float): Average loss.\n",
        "      epoch_acc (float): Accuracy.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = correct / total\n",
        "    return epoch_loss, epoch_acc\n"
      ],
      "metadata": {
        "id": "Q2YQltqyMI8T"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(config=None):\n",
        "    \"\"\"\n",
        "    Runs a single experiment with the given hyperparameters.\n",
        "    - Each experiment gets logged as a separate run on W&B.\n",
        "    - Checkpoints are saved at the end of each epoch with unique filenames and uploaded.\n",
        "    - Optionally, a MultiStepLR scheduler can be used if specified.\n",
        "\n",
        "    Args:\n",
        "      config (dict): A dictionary containing the hyperparameters for the experiment.\n",
        "\n",
        "    Returns:\n",
        "      model (nn.Module): The trained model after the experiment.\n",
        "      test_loader (DataLoader): The DataLoader for the test dataset, ready for evaluation.\n",
        "    \"\"\"\n",
        "    with wandb.init(project=\"-Deep-Learning-Experiments-on-CIFAR-10\", config=config) as run:\n",
        "        config = wandb.config\n",
        "\n",
        "        # Data transformations for training and testing\n",
        "        transform_train = transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "        ])\n",
        "        transform_test = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "        ])\n",
        "\n",
        "        # Download CIFAR-10 dataset\n",
        "        train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "        test_dataset  = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "        # Create a train/validation split (80% train, 20% validation)\n",
        "        num_train = int(0.8 * len(train_dataset))\n",
        "        num_val = len(train_dataset) - num_train\n",
        "        train_subset, val_subset = torch.utils.data.random_split(train_dataset, [num_train, num_val])\n",
        "\n",
        "        train_loader = torch.utils.data.DataLoader(train_subset, batch_size=config.batch_size, shuffle=True, num_workers=2)\n",
        "        val_loader   = torch.utils.data.DataLoader(val_subset, batch_size=config.batch_size, shuffle=False, num_workers=2)\n",
        "        test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "        # Initialize the ResNet-32 model for CIFAR-10\n",
        "        model = get_resnet32_for_cifar10().to(device)\n",
        "\n",
        "        # Define the loss function\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Select the optimizer based on configuration\n",
        "        if config.optimizer.lower() == \"sgd\":\n",
        "            optimizer = optim.SGD(model.parameters(), lr=config.learning_rate, momentum=0.9, weight_decay=5e-4)\n",
        "        elif config.optimizer.lower() == \"adam\":\n",
        "            optimizer = optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=5e-4)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported optimizer type. Please choose 'sgd' or 'adam'.\")\n",
        "\n",
        "        # Optionally, set up a learning rate scheduler\n",
        "        scheduler = None\n",
        "        if hasattr(config, \"use_scheduler\") and config.use_scheduler:\n",
        "            # Decay learning rate at 50% and 75% of total epochs\n",
        "            milestones = [int(config.epochs * 0.5), int(config.epochs * 0.75)]\n",
        "            scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.1)\n",
        "\n",
        "        best_val_acc = 0.0  # To track the best validation accuracy\n",
        "\n",
        "        # Training loop for the specified number of epochs\n",
        "        for epoch in range(config.epochs):\n",
        "            train_loss, train_acc = train_one_epoch(model, optimizer, criterion, train_loader, device)\n",
        "            val_loss, val_acc = validate(model, criterion, val_loader, device)\n",
        "\n",
        "            # Log metrics to W&B for the current epoch\n",
        "            wandb.log({\n",
        "                \"epoch\": epoch + 1,\n",
        "                \"train_loss\": train_loss,\n",
        "                \"train_acc\": train_acc,\n",
        "                \"val_loss\": val_loss,\n",
        "                \"val_acc\": val_acc,\n",
        "                \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
        "                \"optimizer\": config.optimizer\n",
        "            })\n",
        "\n",
        "            print(f\"Epoch [{epoch+1}/{config.epochs}] | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "            # Save every epoch's checkpoint with a unique filename\n",
        "            checkpoint_filename = f\"{wandb.run.id}_epoch_{epoch+1}_{int(time.time())}.pth\"\n",
        "            torch.save(model.state_dict(), checkpoint_filename)\n",
        "            wandb.save(checkpoint_filename)\n",
        "\n",
        "            # Optionally, save the best model checkpoint separately if validation accuracy improves\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                best_checkpoint = f\"{wandb.run.id}_best_epoch_{epoch+1}_{int(time.time())}.pth\"\n",
        "                torch.save(model.state_dict(), best_checkpoint)\n",
        "                wandb.save(best_checkpoint)\n",
        "\n",
        "            # Step the learning rate scheduler, if used\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "\n",
        "        # After training, evaluate the model on the test set\n",
        "        test_loss, test_acc = validate(model, criterion, test_loader, device)\n",
        "        wandb.log({\"test_loss\": test_loss, \"test_acc\": test_acc})\n",
        "        print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "        # Return the trained model and test_loader for further analysis if needed\n",
        "        return model, test_loader\n"
      ],
      "metadata": {
        "id": "TIDKLMSSMKtF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define six hyperparameter configurations\n",
        "experiment_configs = [\n",
        "\n",
        "    {\"epochs\": 10, \"batch_size\": 128, \"learning_rate\": 0.001, \"optimizer\": \"adam\"},\n",
        "    {\"epochs\": 10, \"batch_size\": 128, \"learning_rate\": 0.5, \"optimizer\": \"adam\"},\n",
        "    {\"epochs\": 12, \"batch_size\": 128, \"learning_rate\": 0.0001, \"optimizer\": \"adam\"},\n",
        "    {\"epochs\": 10, \"batch_size\": 128, \"learning_rate\": 0.001, \"optimizer\": \"sgd\"},\n",
        "    {\"epochs\": 10, \"batch_size\": 128, \"learning_rate\": 0.0005, \"optimizer\": \"sgd\"},\n",
        "    {\"epochs\": 85, \"batch_size\": 128, \"learning_rate\": 0.1, \"optimizer\": \"sgd\", \"use_scheduler\": True}\n",
        "]\n",
        "\n",
        "# Run each experiment sequentially\n",
        "for config in experiment_configs:\n",
        "    print(\"\\nStarting experiment with configuration:\")\n",
        "    print(config)\n",
        "    run_experiment(config)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bz3T1P_XMNBc",
        "outputId": "35eafa91-c1a6-4f3e-fb68-7a37083aef7c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting experiment with configuration:\n",
            "{'epochs': 10, 'batch_size': 128, 'learning_rate': 0.001, 'optimizer': 'adam'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250202_102449-v4pqsmut</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10/runs/v4pqsmut' target=\"_blank\">amber-breeze-1</a></strong> to <a href='https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10' target=\"_blank\">https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10/runs/v4pqsmut' target=\"_blank\">https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10/runs/v4pqsmut</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 42.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Epoch [1/10] | Train Loss: 1.5508 | Train Acc: 0.4279 | Val Loss: 1.5323 | Val Acc: 0.4654\n",
            "Epoch [2/10] | Train Loss: 1.1368 | Train Acc: 0.5928 | Val Loss: 1.1430 | Val Acc: 0.5938\n",
            "Epoch [3/10] | Train Loss: 0.9524 | Train Acc: 0.6605 | Val Loss: 1.5651 | Val Acc: 0.5249\n",
            "Epoch [4/10] | Train Loss: 0.8242 | Train Acc: 0.7073 | Val Loss: 0.8743 | Val Acc: 0.6883\n",
            "Epoch [5/10] | Train Loss: 0.7439 | Train Acc: 0.7399 | Val Loss: 0.7422 | Val Acc: 0.7434\n",
            "Epoch [6/10] | Train Loss: 0.6808 | Train Acc: 0.7652 | Val Loss: 0.7802 | Val Acc: 0.7342\n",
            "Epoch [7/10] | Train Loss: 0.6374 | Train Acc: 0.7816 | Val Loss: 0.6525 | Val Acc: 0.7742\n",
            "Epoch [8/10] | Train Loss: 0.6002 | Train Acc: 0.7932 | Val Loss: 0.7998 | Val Acc: 0.7319\n",
            "Epoch [9/10] | Train Loss: 0.5672 | Train Acc: 0.8051 | Val Loss: 0.7070 | Val Acc: 0.7524\n",
            "Epoch [10/10] | Train Loss: 0.5413 | Train Acc: 0.8140 | Val Loss: 0.6838 | Val Acc: 0.7740\n",
            "Test Loss: 0.7260 | Test Acc: 0.7625\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▄▅▆▇▇▇███</td></tr><tr><td>train_loss</td><td>█▅▄▃▂▂▂▁▁▁</td></tr><tr><td>val_acc</td><td>▁▄▂▆▇▇█▇██</td></tr><tr><td>val_loss</td><td>█▅█▃▂▂▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>learning_rate</td><td>0.001</td></tr><tr><td>optimizer</td><td>adam</td></tr><tr><td>test_acc</td><td>0.7625</td></tr><tr><td>test_loss</td><td>0.72604</td></tr><tr><td>train_acc</td><td>0.81398</td></tr><tr><td>train_loss</td><td>0.5413</td></tr><tr><td>val_acc</td><td>0.774</td></tr><tr><td>val_loss</td><td>0.68379</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">amber-breeze-1</strong> at: <a href='https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10/runs/v4pqsmut' target=\"_blank\">https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10/runs/v4pqsmut</a><br> View project at: <a href='https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10' target=\"_blank\">https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 15 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250202_102449-v4pqsmut/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting experiment with configuration:\n",
            "{'epochs': 10, 'batch_size': 128, 'learning_rate': 0.5, 'optimizer': 'adam'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250202_102911-uj4m4c4y</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10/runs/uj4m4c4y' target=\"_blank\">rosy-plant-2</a></strong> to <a href='https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10' target=\"_blank\">https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10/runs/uj4m4c4y' target=\"_blank\">https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10/runs/uj4m4c4y</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch [1/10] | Train Loss: 2.5328 | Train Acc: 0.1548 | Val Loss: 5.0554 | Val Acc: 0.1093\n",
            "Epoch [2/10] | Train Loss: 2.3884 | Train Acc: 0.1242 | Val Loss: 784.5759 | Val Acc: 0.0969\n",
            "Epoch [3/10] | Train Loss: 2.5197 | Train Acc: 0.1028 | Val Loss: 5.6845 | Val Acc: 0.1016\n",
            "Epoch [4/10] | Train Loss: 2.6137 | Train Acc: 0.1056 | Val Loss: 473.1868 | Val Acc: 0.0982\n",
            "Epoch [5/10] | Train Loss: 2.4229 | Train Acc: 0.1092 | Val Loss: 3.1442 | Val Acc: 0.0997\n",
            "Epoch [6/10] | Train Loss: 2.3845 | Train Acc: 0.1018 | Val Loss: 5668.8476 | Val Acc: 0.1016\n",
            "Epoch [7/10] | Train Loss: 2.3593 | Train Acc: 0.1192 | Val Loss: 2.3155 | Val Acc: 0.0977\n",
            "Epoch [8/10] | Train Loss: 2.4367 | Train Acc: 0.1110 | Val Loss: 58.4995 | Val Acc: 0.0988\n",
            "Epoch [9/10] | Train Loss: 2.4514 | Train Acc: 0.1056 | Val Loss: 3.5730 | Val Acc: 0.1071\n",
            "Epoch [10/10] | Train Loss: 2.5021 | Train Acc: 0.1045 | Val Loss: 8034.9673 | Val Acc: 0.0997\n",
            "Test Loss: 7923.1030 | Test Acc: 0.1000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_acc</td><td>█▄▁▂▂▁▃▂▁▁</td></tr><tr><td>train_loss</td><td>▆▂▅█▃▂▁▃▄▅</td></tr><tr><td>val_acc</td><td>█▁▄▂▃▄▁▂▇▃</td></tr><tr><td>val_loss</td><td>▁▂▁▁▁▆▁▁▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>learning_rate</td><td>0.5</td></tr><tr><td>optimizer</td><td>adam</td></tr><tr><td>test_acc</td><td>0.1</td></tr><tr><td>test_loss</td><td>7923.10298</td></tr><tr><td>train_acc</td><td>0.1045</td></tr><tr><td>train_loss</td><td>2.50207</td></tr><tr><td>val_acc</td><td>0.0997</td></tr><tr><td>val_loss</td><td>8034.96725</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">rosy-plant-2</strong> at: <a href='https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10/runs/uj4m4c4y' target=\"_blank\">https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10/runs/uj4m4c4y</a><br> View project at: <a href='https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10' target=\"_blank\">https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 11 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250202_102911-uj4m4c4y/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting experiment with configuration:\n",
            "{'epochs': 12, 'batch_size': 128, 'learning_rate': 0.0001, 'optimizer': 'adam'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250202_103320-r874j3nc</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10/runs/r874j3nc' target=\"_blank\">clean-bee-3</a></strong> to <a href='https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10' target=\"_blank\">https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10/runs/r874j3nc' target=\"_blank\">https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10/runs/r874j3nc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch [1/12] | Train Loss: 1.7752 | Train Acc: 0.3370 | Val Loss: 1.5943 | Val Acc: 0.4186\n",
            "Epoch [2/12] | Train Loss: 1.4429 | Train Acc: 0.4693 | Val Loss: 1.3971 | Val Acc: 0.4893\n",
            "Epoch [3/12] | Train Loss: 1.3035 | Train Acc: 0.5298 | Val Loss: 1.3606 | Val Acc: 0.5093\n",
            "Epoch [4/12] | Train Loss: 1.1968 | Train Acc: 0.5670 | Val Loss: 1.1728 | Val Acc: 0.5768\n",
            "Epoch [5/12] | Train Loss: 1.1040 | Train Acc: 0.6046 | Val Loss: 1.1120 | Val Acc: 0.6043\n",
            "Epoch [6/12] | Train Loss: 1.0371 | Train Acc: 0.6299 | Val Loss: 1.0386 | Val Acc: 0.6313\n",
            "Epoch [7/12] | Train Loss: 0.9802 | Train Acc: 0.6536 | Val Loss: 0.9859 | Val Acc: 0.6485\n",
            "Epoch [8/12] | Train Loss: 0.9303 | Train Acc: 0.6707 | Val Loss: 0.9353 | Val Acc: 0.6651\n",
            "Epoch [9/12] | Train Loss: 0.8852 | Train Acc: 0.6897 | Val Loss: 0.9361 | Val Acc: 0.6722\n",
            "Epoch [10/12] | Train Loss: 0.8420 | Train Acc: 0.7038 | Val Loss: 0.8800 | Val Acc: 0.6864\n",
            "Epoch [11/12] | Train Loss: 0.8029 | Train Acc: 0.7166 | Val Loss: 0.8854 | Val Acc: 0.6861\n",
            "Epoch [12/12] | Train Loss: 0.7662 | Train Acc: 0.7304 | Val Loss: 0.7962 | Val Acc: 0.7175\n",
            "Test Loss: 0.7727 | Test Acc: 0.7293\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▄▄▅▅▆▇▇█</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▃▄▅▆▆▇▇▇███</td></tr><tr><td>train_loss</td><td>█▆▅▄▃▃▂▂▂▂▁▁</td></tr><tr><td>val_acc</td><td>▁▃▃▅▅▆▆▇▇▇▇█</td></tr><tr><td>val_loss</td><td>█▆▆▄▄▃▃▂▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>12</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>optimizer</td><td>adam</td></tr><tr><td>test_acc</td><td>0.7293</td></tr><tr><td>test_loss</td><td>0.77273</td></tr><tr><td>train_acc</td><td>0.73037</td></tr><tr><td>train_loss</td><td>0.76621</td></tr><tr><td>val_acc</td><td>0.7175</td></tr><tr><td>val_loss</td><td>0.79617</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">clean-bee-3</strong> at: <a href='https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10/runs/r874j3nc' target=\"_blank\">https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10/runs/r874j3nc</a><br> View project at: <a href='https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10' target=\"_blank\">https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 23 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250202_103320-r874j3nc/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting experiment with configuration:\n",
            "{'epochs': 10, 'batch_size': 128, 'learning_rate': 0.001, 'optimizer': 'sgd'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250202_103821-gkdosadr</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10/runs/gkdosadr' target=\"_blank\">different-haze-4</a></strong> to <a href='https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10' target=\"_blank\">https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10/runs/gkdosadr' target=\"_blank\">https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10/runs/gkdosadr</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch [1/10] | Train Loss: 1.8385 | Train Acc: 0.3063 | Val Loss: 1.6675 | Val Acc: 0.3742\n",
            "Epoch [2/10] | Train Loss: 1.5326 | Train Acc: 0.4299 | Val Loss: 1.5137 | Val Acc: 0.4442\n",
            "Epoch [3/10] | Train Loss: 1.3640 | Train Acc: 0.4999 | Val Loss: 1.3207 | Val Acc: 0.5189\n",
            "Epoch [4/10] | Train Loss: 1.2282 | Train Acc: 0.5543 | Val Loss: 1.2100 | Val Acc: 0.5633\n",
            "Epoch [5/10] | Train Loss: 1.1227 | Train Acc: 0.5918 | Val Loss: 1.1520 | Val Acc: 0.5916\n",
            "Epoch [6/10] | Train Loss: 1.0515 | Train Acc: 0.6204 | Val Loss: 1.0147 | Val Acc: 0.6336\n",
            "Epoch [7/10] | Train Loss: 0.9866 | Train Acc: 0.6454 | Val Loss: 0.9891 | Val Acc: 0.6448\n",
            "Epoch [8/10] | Train Loss: 0.9312 | Train Acc: 0.6681 | Val Loss: 1.0441 | Val Acc: 0.6327\n",
            "Epoch [9/10] | Train Loss: 0.8886 | Train Acc: 0.6804 | Val Loss: 0.8978 | Val Acc: 0.6795\n",
            "Epoch [10/10] | Train Loss: 0.8465 | Train Acc: 0.6964 | Val Loss: 0.8696 | Val Acc: 0.6900\n",
            "Test Loss: 0.8472 | Test Acc: 0.6980\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▃▄▅▆▇▇▇██</td></tr><tr><td>train_loss</td><td>█▆▅▄▃▂▂▂▁▁</td></tr><tr><td>val_acc</td><td>▁▃▄▅▆▇▇▇██</td></tr><tr><td>val_loss</td><td>█▇▅▄▃▂▂▃▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>learning_rate</td><td>0.001</td></tr><tr><td>optimizer</td><td>sgd</td></tr><tr><td>test_acc</td><td>0.698</td></tr><tr><td>test_loss</td><td>0.84723</td></tr><tr><td>train_acc</td><td>0.69635</td></tr><tr><td>train_loss</td><td>0.84654</td></tr><tr><td>val_acc</td><td>0.69</td></tr><tr><td>val_loss</td><td>0.86957</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">different-haze-4</strong> at: <a href='https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10/runs/gkdosadr' target=\"_blank\">https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10/runs/gkdosadr</a><br> View project at: <a href='https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10' target=\"_blank\">https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 19 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250202_103821-gkdosadr/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting experiment with configuration:\n",
            "{'epochs': 10, 'batch_size': 128, 'learning_rate': 0.0005, 'optimizer': 'sgd'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250202_104235-t86lqtmt</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10/runs/t86lqtmt' target=\"_blank\">woven-snowball-5</a></strong> to <a href='https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10' target=\"_blank\">https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10/runs/t86lqtmt' target=\"_blank\">https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10/runs/t86lqtmt</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch [1/10] | Train Loss: 1.9991 | Train Acc: 0.2462 | Val Loss: 1.7891 | Val Acc: 0.3264\n",
            "Epoch [2/10] | Train Loss: 1.6866 | Train Acc: 0.3597 | Val Loss: 1.6005 | Val Acc: 0.3953\n",
            "Epoch [3/10] | Train Loss: 1.5536 | Train Acc: 0.4183 | Val Loss: 1.4921 | Val Acc: 0.4473\n",
            "Epoch [4/10] | Train Loss: 1.4416 | Train Acc: 0.4661 | Val Loss: 1.3856 | Val Acc: 0.4904\n",
            "Epoch [5/10] | Train Loss: 1.3504 | Train Acc: 0.5081 | Val Loss: 1.3110 | Val Acc: 0.5219\n",
            "Epoch [6/10] | Train Loss: 1.2673 | Train Acc: 0.5388 | Val Loss: 1.2616 | Val Acc: 0.5442\n",
            "Epoch [7/10] | Train Loss: 1.1931 | Train Acc: 0.5696 | Val Loss: 1.1618 | Val Acc: 0.5810\n",
            "Epoch [8/10] | Train Loss: 1.1273 | Train Acc: 0.5923 | Val Loss: 1.1462 | Val Acc: 0.5921\n",
            "Epoch [9/10] | Train Loss: 1.0773 | Train Acc: 0.6118 | Val Loss: 1.0735 | Val Acc: 0.6111\n",
            "Epoch [10/10] | Train Loss: 1.0296 | Train Acc: 0.6296 | Val Loss: 1.0609 | Val Acc: 0.6186\n",
            "Test Loss: 1.0624 | Test Acc: 0.6260\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▃▄▅▆▆▇▇██</td></tr><tr><td>train_loss</td><td>█▆▅▄▃▃▂▂▁▁</td></tr><tr><td>val_acc</td><td>▁▃▄▅▆▆▇▇██</td></tr><tr><td>val_loss</td><td>█▆▅▄▃▃▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>learning_rate</td><td>0.0005</td></tr><tr><td>optimizer</td><td>sgd</td></tr><tr><td>test_acc</td><td>0.626</td></tr><tr><td>test_loss</td><td>1.06244</td></tr><tr><td>train_acc</td><td>0.62962</td></tr><tr><td>train_loss</td><td>1.02955</td></tr><tr><td>val_acc</td><td>0.6186</td></tr><tr><td>val_loss</td><td>1.06089</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">woven-snowball-5</strong> at: <a href='https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10/runs/t86lqtmt' target=\"_blank\">https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10/runs/t86lqtmt</a><br> View project at: <a href='https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10' target=\"_blank\">https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 20 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250202_104235-t86lqtmt/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting experiment with configuration:\n",
            "{'epochs': 85, 'batch_size': 128, 'learning_rate': 0.1, 'optimizer': 'sgd', 'use_scheduler': True}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250202_104645-gi54t6b6</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10/runs/gi54t6b6' target=\"_blank\">floral-surf-6</a></strong> to <a href='https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10' target=\"_blank\">https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10/runs/gi54t6b6' target=\"_blank\">https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10/runs/gi54t6b6</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch [1/85] | Train Loss: 1.7343 | Train Acc: 0.3404 | Val Loss: 1.6176 | Val Acc: 0.4206\n",
            "Epoch [2/85] | Train Loss: 1.2795 | Train Acc: 0.5305 | Val Loss: 1.4747 | Val Acc: 0.4892\n",
            "Epoch [3/85] | Train Loss: 1.0557 | Train Acc: 0.6208 | Val Loss: 1.1576 | Val Acc: 0.5899\n",
            "Epoch [4/85] | Train Loss: 0.9173 | Train Acc: 0.6742 | Val Loss: 1.6458 | Val Acc: 0.5254\n",
            "Epoch [5/85] | Train Loss: 0.8090 | Train Acc: 0.7176 | Val Loss: 0.8790 | Val Acc: 0.6934\n",
            "Epoch [6/85] | Train Loss: 0.7377 | Train Acc: 0.7439 | Val Loss: 0.8946 | Val Acc: 0.7028\n",
            "Epoch [7/85] | Train Loss: 0.6943 | Train Acc: 0.7593 | Val Loss: 0.9603 | Val Acc: 0.6786\n",
            "Epoch [8/85] | Train Loss: 0.6570 | Train Acc: 0.7735 | Val Loss: 0.8080 | Val Acc: 0.7197\n",
            "Epoch [9/85] | Train Loss: 0.6270 | Train Acc: 0.7829 | Val Loss: 0.8135 | Val Acc: 0.7247\n",
            "Epoch [10/85] | Train Loss: 0.6016 | Train Acc: 0.7931 | Val Loss: 1.1728 | Val Acc: 0.6457\n",
            "Epoch [11/85] | Train Loss: 0.5895 | Train Acc: 0.7984 | Val Loss: 0.7506 | Val Acc: 0.7448\n",
            "Epoch [12/85] | Train Loss: 0.5732 | Train Acc: 0.8033 | Val Loss: 0.7340 | Val Acc: 0.7460\n",
            "Epoch [13/85] | Train Loss: 0.5537 | Train Acc: 0.8093 | Val Loss: 0.8313 | Val Acc: 0.7265\n",
            "Epoch [14/85] | Train Loss: 0.5471 | Train Acc: 0.8135 | Val Loss: 0.6116 | Val Acc: 0.7939\n",
            "Epoch [15/85] | Train Loss: 0.5289 | Train Acc: 0.8211 | Val Loss: 1.0732 | Val Acc: 0.6539\n",
            "Epoch [16/85] | Train Loss: 0.5216 | Train Acc: 0.8219 | Val Loss: 0.6955 | Val Acc: 0.7675\n",
            "Epoch [17/85] | Train Loss: 0.5233 | Train Acc: 0.8204 | Val Loss: 0.6585 | Val Acc: 0.7708\n",
            "Epoch [18/85] | Train Loss: 0.5135 | Train Acc: 0.8246 | Val Loss: 0.9340 | Val Acc: 0.7286\n",
            "Epoch [19/85] | Train Loss: 0.4983 | Train Acc: 0.8291 | Val Loss: 0.8506 | Val Acc: 0.7215\n",
            "Epoch [20/85] | Train Loss: 0.4947 | Train Acc: 0.8306 | Val Loss: 0.6139 | Val Acc: 0.7893\n",
            "Epoch [21/85] | Train Loss: 0.4932 | Train Acc: 0.8300 | Val Loss: 0.6121 | Val Acc: 0.7932\n",
            "Epoch [22/85] | Train Loss: 0.4897 | Train Acc: 0.8297 | Val Loss: 0.6164 | Val Acc: 0.7884\n",
            "Epoch [23/85] | Train Loss: 0.4836 | Train Acc: 0.8351 | Val Loss: 0.7015 | Val Acc: 0.7670\n",
            "Epoch [24/85] | Train Loss: 0.4812 | Train Acc: 0.8345 | Val Loss: 0.8105 | Val Acc: 0.7242\n",
            "Epoch [25/85] | Train Loss: 0.4679 | Train Acc: 0.8402 | Val Loss: 0.6347 | Val Acc: 0.7880\n",
            "Epoch [26/85] | Train Loss: 0.4718 | Train Acc: 0.8368 | Val Loss: 0.8173 | Val Acc: 0.7318\n",
            "Epoch [27/85] | Train Loss: 0.4658 | Train Acc: 0.8394 | Val Loss: 0.5776 | Val Acc: 0.7976\n",
            "Epoch [28/85] | Train Loss: 0.4621 | Train Acc: 0.8401 | Val Loss: 0.7046 | Val Acc: 0.7645\n",
            "Epoch [29/85] | Train Loss: 0.4525 | Train Acc: 0.8460 | Val Loss: 1.1288 | Val Acc: 0.6659\n",
            "Epoch [30/85] | Train Loss: 0.4569 | Train Acc: 0.8422 | Val Loss: 0.5946 | Val Acc: 0.8026\n",
            "Epoch [31/85] | Train Loss: 0.4533 | Train Acc: 0.8448 | Val Loss: 0.6862 | Val Acc: 0.7651\n",
            "Epoch [32/85] | Train Loss: 0.4472 | Train Acc: 0.8484 | Val Loss: 0.5852 | Val Acc: 0.8027\n",
            "Epoch [33/85] | Train Loss: 0.4463 | Train Acc: 0.8479 | Val Loss: 0.5984 | Val Acc: 0.7941\n",
            "Epoch [34/85] | Train Loss: 0.4465 | Train Acc: 0.8471 | Val Loss: 0.7743 | Val Acc: 0.7417\n",
            "Epoch [35/85] | Train Loss: 0.4366 | Train Acc: 0.8516 | Val Loss: 0.5379 | Val Acc: 0.8150\n",
            "Epoch [36/85] | Train Loss: 0.4420 | Train Acc: 0.8493 | Val Loss: 0.6200 | Val Acc: 0.7863\n",
            "Epoch [37/85] | Train Loss: 0.4391 | Train Acc: 0.8488 | Val Loss: 0.7634 | Val Acc: 0.7502\n",
            "Epoch [38/85] | Train Loss: 0.4378 | Train Acc: 0.8498 | Val Loss: 0.6940 | Val Acc: 0.7743\n",
            "Epoch [39/85] | Train Loss: 0.4352 | Train Acc: 0.8503 | Val Loss: 0.5715 | Val Acc: 0.8055\n",
            "Epoch [40/85] | Train Loss: 0.4354 | Train Acc: 0.8501 | Val Loss: 0.5269 | Val Acc: 0.8197\n",
            "Epoch [41/85] | Train Loss: 0.4259 | Train Acc: 0.8516 | Val Loss: 0.6845 | Val Acc: 0.7752\n",
            "Epoch [42/85] | Train Loss: 0.4285 | Train Acc: 0.8527 | Val Loss: 0.5395 | Val Acc: 0.8152\n",
            "Epoch [43/85] | Train Loss: 0.2720 | Train Acc: 0.9069 | Val Loss: 0.3030 | Val Acc: 0.8916\n",
            "Epoch [44/85] | Train Loss: 0.2272 | Train Acc: 0.9217 | Val Loss: 0.3026 | Val Acc: 0.8942\n",
            "Epoch [45/85] | Train Loss: 0.2048 | Train Acc: 0.9303 | Val Loss: 0.2876 | Val Acc: 0.9022\n",
            "Epoch [46/85] | Train Loss: 0.1937 | Train Acc: 0.9346 | Val Loss: 0.2867 | Val Acc: 0.9023\n",
            "Epoch [47/85] | Train Loss: 0.1848 | Train Acc: 0.9368 | Val Loss: 0.2733 | Val Acc: 0.9066\n",
            "Epoch [48/85] | Train Loss: 0.1754 | Train Acc: 0.9406 | Val Loss: 0.2856 | Val Acc: 0.9029\n",
            "Epoch [49/85] | Train Loss: 0.1672 | Train Acc: 0.9420 | Val Loss: 0.2862 | Val Acc: 0.9045\n",
            "Epoch [50/85] | Train Loss: 0.1597 | Train Acc: 0.9453 | Val Loss: 0.2847 | Val Acc: 0.9058\n",
            "Epoch [51/85] | Train Loss: 0.1517 | Train Acc: 0.9484 | Val Loss: 0.2886 | Val Acc: 0.9073\n",
            "Epoch [52/85] | Train Loss: 0.1487 | Train Acc: 0.9487 | Val Loss: 0.2767 | Val Acc: 0.9088\n",
            "Epoch [53/85] | Train Loss: 0.1393 | Train Acc: 0.9526 | Val Loss: 0.2793 | Val Acc: 0.9063\n",
            "Epoch [54/85] | Train Loss: 0.1351 | Train Acc: 0.9537 | Val Loss: 0.2962 | Val Acc: 0.9028\n",
            "Epoch [55/85] | Train Loss: 0.1371 | Train Acc: 0.9528 | Val Loss: 0.2899 | Val Acc: 0.9040\n",
            "Epoch [56/85] | Train Loss: 0.1301 | Train Acc: 0.9557 | Val Loss: 0.3039 | Val Acc: 0.8994\n",
            "Epoch [57/85] | Train Loss: 0.1248 | Train Acc: 0.9576 | Val Loss: 0.3068 | Val Acc: 0.9018\n",
            "Epoch [58/85] | Train Loss: 0.1188 | Train Acc: 0.9597 | Val Loss: 0.3200 | Val Acc: 0.8974\n",
            "Epoch [59/85] | Train Loss: 0.1220 | Train Acc: 0.9587 | Val Loss: 0.2954 | Val Acc: 0.9037\n",
            "Epoch [60/85] | Train Loss: 0.1221 | Train Acc: 0.9579 | Val Loss: 0.3097 | Val Acc: 0.8992\n",
            "Epoch [61/85] | Train Loss: 0.1188 | Train Acc: 0.9581 | Val Loss: 0.3139 | Val Acc: 0.9033\n",
            "Epoch [62/85] | Train Loss: 0.1127 | Train Acc: 0.9619 | Val Loss: 0.2965 | Val Acc: 0.9033\n",
            "Epoch [63/85] | Train Loss: 0.1098 | Train Acc: 0.9624 | Val Loss: 0.3223 | Val Acc: 0.9028\n",
            "Epoch [64/85] | Train Loss: 0.0807 | Train Acc: 0.9738 | Val Loss: 0.2723 | Val Acc: 0.9145\n",
            "Epoch [65/85] | Train Loss: 0.0695 | Train Acc: 0.9779 | Val Loss: 0.2705 | Val Acc: 0.9138\n",
            "Epoch [66/85] | Train Loss: 0.0654 | Train Acc: 0.9801 | Val Loss: 0.2594 | Val Acc: 0.9178\n",
            "Epoch [67/85] | Train Loss: 0.0595 | Train Acc: 0.9828 | Val Loss: 0.2689 | Val Acc: 0.9188\n",
            "Epoch [68/85] | Train Loss: 0.0578 | Train Acc: 0.9828 | Val Loss: 0.2771 | Val Acc: 0.9160\n",
            "Epoch [69/85] | Train Loss: 0.0561 | Train Acc: 0.9835 | Val Loss: 0.2668 | Val Acc: 0.9182\n",
            "Epoch [70/85] | Train Loss: 0.0551 | Train Acc: 0.9842 | Val Loss: 0.2738 | Val Acc: 0.9152\n",
            "Epoch [71/85] | Train Loss: 0.0534 | Train Acc: 0.9839 | Val Loss: 0.2742 | Val Acc: 0.9172\n",
            "Epoch [72/85] | Train Loss: 0.0526 | Train Acc: 0.9844 | Val Loss: 0.2781 | Val Acc: 0.9160\n",
            "Epoch [73/85] | Train Loss: 0.0503 | Train Acc: 0.9852 | Val Loss: 0.2699 | Val Acc: 0.9194\n",
            "Epoch [74/85] | Train Loss: 0.0463 | Train Acc: 0.9870 | Val Loss: 0.2660 | Val Acc: 0.9194\n",
            "Epoch [75/85] | Train Loss: 0.0477 | Train Acc: 0.9857 | Val Loss: 0.2793 | Val Acc: 0.9151\n",
            "Epoch [76/85] | Train Loss: 0.0466 | Train Acc: 0.9865 | Val Loss: 0.2769 | Val Acc: 0.9162\n",
            "Epoch [77/85] | Train Loss: 0.0452 | Train Acc: 0.9867 | Val Loss: 0.2725 | Val Acc: 0.9142\n",
            "Epoch [78/85] | Train Loss: 0.0439 | Train Acc: 0.9873 | Val Loss: 0.2803 | Val Acc: 0.9154\n",
            "Epoch [79/85] | Train Loss: 0.0425 | Train Acc: 0.9877 | Val Loss: 0.2867 | Val Acc: 0.9152\n",
            "Epoch [80/85] | Train Loss: 0.0411 | Train Acc: 0.9882 | Val Loss: 0.2807 | Val Acc: 0.9164\n",
            "Epoch [81/85] | Train Loss: 0.0409 | Train Acc: 0.9880 | Val Loss: 0.2816 | Val Acc: 0.9168\n",
            "Epoch [82/85] | Train Loss: 0.0412 | Train Acc: 0.9882 | Val Loss: 0.2806 | Val Acc: 0.9155\n",
            "Epoch [83/85] | Train Loss: 0.0395 | Train Acc: 0.9894 | Val Loss: 0.2865 | Val Acc: 0.9142\n",
            "Epoch [84/85] | Train Loss: 0.0396 | Train Acc: 0.9891 | Val Loss: 0.2877 | Val Acc: 0.9148\n",
            "Epoch [85/85] | Train Loss: 0.0382 | Train Acc: 0.9894 | Val Loss: 0.2861 | Val Acc: 0.9178\n",
            "Test Loss: 0.3071 | Test Acc: 0.9162\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇█████</td></tr><tr><td>learning_rate</td><td>█████████████████████▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▃▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇████████████████</td></tr><tr><td>train_loss</td><td>█▆▅▅▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▂▁▄▄▄▃▅▅▅▃▄▆▆▅▅▄▆▅▆▆▅███████████████████</td></tr><tr><td>val_loss</td><td>▇█▄▅▄▄▃▃▃▄▄▃▅▃▃▄▂▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>85</td></tr><tr><td>learning_rate</td><td>0.001</td></tr><tr><td>optimizer</td><td>sgd</td></tr><tr><td>test_acc</td><td>0.9162</td></tr><tr><td>test_loss</td><td>0.30706</td></tr><tr><td>train_acc</td><td>0.98938</td></tr><tr><td>train_loss</td><td>0.03818</td></tr><tr><td>val_acc</td><td>0.9178</td></tr><tr><td>val_loss</td><td>0.28606</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">floral-surf-6</strong> at: <a href='https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10/runs/gi54t6b6' target=\"_blank\">https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10/runs/gi54t6b6</a><br> View project at: <a href='https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10' target=\"_blank\">https://wandb.ai/gauravengineer85-vellore-institute-of-technology/-Deep-Learning-Experiments-on-CIFAR-10</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 111 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250202_104645-gi54t6b6/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Initialize the model architecture (must match the saved model's architecture)\n",
        "model = get_resnet32_for_cifar10().to(device)\n",
        "\n",
        "\n",
        "model.load_state_dict(torch.load(\"/content/gi54t6b6_best_epoch_11_1738493475.pth\", map_location=device))\n",
        "model.eval()  # Set to evaluation mode\n",
        "\n",
        "print(\"Model successfully loaded and ready for predictions!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQw-ePSUMQNf",
        "outputId": "1d5cce5a-386b-47e1-d11a-fbffb9124046"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model successfully loaded and ready for predictions!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-df7f91225621>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"/content/gi54t6b6_best_epoch_11_1738493475.pth\", map_location=device))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "])\n",
        "\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "print(\" Test dataset loaded successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_OgCuTfZYut",
        "outputId": "3357d85b-ad9d-4af9-8ba9-7bc5d2f3b524"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            " Test dataset loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def visualize_predictions(model, data_loader, class_names, num_images=5):\n",
        "    \"\"\"Visualizes predictions from a model on a given data loader.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The trained model.\n",
        "        data_loader (DataLoader): The data loader to get images and labels from.\n",
        "        class_names (list): A list of class names for the dataset.\n",
        "        num_images (int, optional): The number of images to visualize. Defaults to 5.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get some images and labels from the data loader\n",
        "    images, labels = next(iter(data_loader))\n",
        "    images, labels = images[:num_images].to(device), labels[:num_images].to(device)\n",
        "\n",
        "    # Make predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "    # Display images with predicted and actual labels\n",
        "    fig, axes = plt.subplots(1, num_images, figsize=(15, 3))\n",
        "    for i in range(num_images):\n",
        "        ax = axes[i]\n",
        "        ax.imshow(np.transpose(images[i].cpu().numpy(), (1, 2, 0)))  # Transpose for correct display\n",
        "        ax.set_title(f\"Predicted: {class_names[predicted[i]]}\\nActual: {class_names[labels[i]]}\")\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "6l-vw9S-Zbfm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  CIFAR-10 class names\n",
        "cifar10_class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "                        'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# Run the prediction function\n",
        "visualize_predictions(model, test_loader, cifar10_class_names, num_images=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "JctqFwohZeHK",
        "outputId": "1e84cc34-f6b3-4624-9308-4660b0c3f02c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.7830755..2.0985878].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.9894737..2.0004883].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis and Insights Report\n",
        "\n",
        "## 1. Overview\n",
        "This report evaluates the **CIFAR-10 classification task using ResNet-32**, trained across multiple experiments with different hyperparameters. The results were tracked using **Weights & Biases (W&B)** to compare model performance, training efficiency, and hyperparameter effectiveness.\n",
        "\n",
        "### Key Performance Metrics:\n",
        "| Run Name         | Optimizer | Learning Rate | Epochs | Test Accuracy | Train Accuracy | Validation Accuracy |\n",
        "|-----------------|-----------|--------------|--------|--------------|--------------|-------------------|\n",
        "| floral-surf-6   | SGD       | 0.1          | 85     | **91.62%**   | **98.94%**   | **91.78%**        |\n",
        "| woven-snowball-5 | SGD      | 0.0005       | 10     | 62.63%       | 62.96%       | 61.86%            |\n",
        "| different-haze-4 | SGD      | 0.001        | 10     | 69.82%       | 69.63%       | 69.00%            |\n",
        "| clean-bee-3     | Adam      | 0.0001       | 12     | 72.93%       | 73.03%       | 71.75%            |\n",
        "| rosy-plant-2    | Adam      | 0.5          | 10     | **10.04%**   | **10.45%**   | **9.97%**         |\n",
        "| amber-breeze-1  | Adam      | 0.001        | 10     | 76.25%       | 81.39%       | 77.40%            |\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Experiment Comparisons Using W&B Dashboards\n",
        "### Key Observations:\n",
        "- **Higher learning rates (0.5) caused extreme instability**, as seen in `rosy-plant-2` with a **test accuracy of only 10.04%**.\n",
        "- **SGD with LR scheduling (`floral-surf-6`) provided the best performance**, achieving **91.62% test accuracy**.\n",
        "- **Adam optimizer performed well for quick convergence**, but **plateaued around 76.25%** in `amber-breeze-1`.\n",
        "- **Training with fewer epochs resulted in underperformance**, highlighting the need for longer training runs.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. How Hyperparameters Affected Performance\n",
        "\n",
        "### **Learning Rate (LR) Impact:**\n",
        "- **Best LR:** **0.1 (with decay at 50 & 75 epochs)** → Enabled **faster convergence and high accuracy**.\n",
        "- **Worst LR:** **0.5** → Training was highly unstable, causing **extremely high loss and poor test accuracy (10.04%)**.\n",
        "\n",
        "### **Optimizer Comparison: SGD vs. Adam**\n",
        "- **SGD (momentum) outperformed Adam in longer runs**.\n",
        "- **Adam was effective in short runs but plateaued below 80% accuracy**.\n",
        "\n",
        "### **Effect of Training Epochs:**\n",
        "- **Less than 20 epochs led to underfitting**, with accuracy **below 70%** in most cases.\n",
        "- **85 epochs provided strong generalization**, as seen in `floral-surf-6`.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Best Experiment & Why?\n",
        "### Best Performing Run: `floral-surf-6`\n",
        "- **Optimizer:** SGD (Momentum = 0.9)\n",
        "- **Learning Rate:** 0.1 (with decay at 50 & 75 epochs)\n",
        "- **Epochs:** 85\n",
        "- **Test Accuracy:** **91.62%**\n",
        "- **Validation Accuracy:** **91.78%**\n",
        "\n",
        "### Why was it the best?\n",
        "- **Effective LR Scheduling:** Allowed controlled optimization and smooth convergence.\n",
        "- **SGD with momentum stabilized training**, reducing variance in updates.\n",
        "- **Longer training (85 epochs) ensured strong feature extraction.**\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Summary & Final Insights\n",
        "- **SGD with momentum works best for longer training runs (85+ epochs).**  \n",
        "- **High learning rates without decay lead to extreme instability.**  \n",
        "- **Adam optimizer is effective but saturates below 80% accuracy.**  \n",
        "- **The best accuracy achieved = 91.62% (`floral-surf-6`).**  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nBlZesDPbruj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G9CzOnPHZylT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}